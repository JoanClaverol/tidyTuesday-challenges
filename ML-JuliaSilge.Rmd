---
title: "TidyTuesday ML from Julia Silge"
output: 
  html_document: 
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
                      cache = TRUE)
```

## The Office 

We are going to predict the Imdb rating for the Office episodes. To do so, we will use a lasso regression,

### Lasso regression 

It is a regularisation technique. *Regularizations are techniques used to reduce the error by fitting a function appropriately on the given training set and avoid overfitting.* [...] *In machine learning, you will have a fair idea that regularization penalizes the coefficients. In deep learning, it actually penalizes the weight matrices of the nodes.* 

Why do we use it? 

* Lasso regression helps reduce overfitting and it is particularly useful for feature selection. 
* Lasso regression can be useful if we have several independent variables that are useless. 
* Ridge regression can reduce the slope close to zero (but not exactly zero) but Lasso regression can reduce the slope to be exactly equal to zero. 

### Tidytuesday challenge

```{r}
library(tidyverse)
library(tidytuesdayR)

theme_set(theme_light())

office_data <- tt_load("2020-03-17")
ratings_raw <- office_data$office_ratings 

# prepare preprocess to join office data from tidytuesday challenge, to the 
# office data from the library schrute
remove_regex <- "[:punct:]|[:digit:]|parts |part |the |and" # some episodes are
# part 1 and others are 1 and then there is another on which is 2
office_ratings <- ratings_raw %>%
  transmute(
    episode_name = str_to_lower(title),
    episode_name = str_remove_all(episode_name, remove_regex),
    episode_name = str_trim(episode_name),
    imdb_rating
  )

office_info <- schrute::theoffice %>%
  mutate(
    season = as.numeric(season),
    episode = as.numeric(episode),
    episode_name = str_to_lower(episode_name),
    episode_name = str_remove_all(episode_name, remove_regex),
    episode_name = str_trim(episode_name)
  ) %>%
  select(season, episode, episode_name, director, writer, character)

```

Now we are going to build the dataset to start modeling: 

```{r}
# let's see how many line do we have for each character for each episode
characters <- office_info %>%
  count(episode_name, character) %>%
  add_count(character, wt = n, name = "character_count") %>%
  filter(character_count > 800) %>%
  select(-character_count) %>%
  pivot_wider(
    names_from = character,
    values_from = n,
    values_fill = list(n = 0)
  )
```

Let's focus on the writer and the director: 

```{r}
creators <- office_info %>%
  distinct(episode_name, director, writer) %>%
  pivot_longer(director:writer, names_to = "role", values_to = "person") %>%
  separate_rows(person, sep = ";") %>%
  add_count(person) %>%
  filter(n > 10) %>%
  distinct(episode_name, person) %>%
  mutate(person_value = 1) %>%
  pivot_wider(
    names_from = person,
    values_from = person_value,
    values_fill = list(person_value = 0)
  )
```

Now we are going to join the information: 

```{r}
office <- office_info %>%
  distinct(season, episode, episode_name) %>%
  inner_join(characters) %>%
  inner_join(creators) %>%
  inner_join(office_ratings %>%
    select(episode_name, imdb_rating)) %>%
  janitor::clean_names()
```

Show some visualizations for the ratings of the different seasons: 

```{r}
office %>% 
  ggplot(aes(season, imdb_rating, fill = as.factor(season))) + 
    geom_boxplot(show.legend = FALSE)
```

And let's see of there is a high ratings at the end of the seasons: 

```{r}
office %>% 
  ggplot(aes(episode, imdb_rating, fill = as.factor(episode))) + 
    geom_boxplot(show.legend = FALSE)
```


#### Train a model 

What we want to understand if the appearance of an specific actor it is making the episode more highly to have a high rating. 

```{r}
library(tidymodels)

# let's do our data split 
office_split <- initial_split(office, strata = season) # we want to have the same proportion of episodes from different seasons in the test that in the train 
office_train <- training(office_split)
office_test <- testing(office_split)
# small test

# now it is time to write the recipe
office_rec <- recipe(imdb_rating ~ ., data = office_train) %>%
  # we want to keep the episode name, as we are going to use it as id, but
  # we don't want to use it to predict
  update_role(episode_name, new_role = "ID") %>%
  # Will remove variables that contains only a single value, so removing 
  # 0 variance
  step_zv(all_numeric(), -all_outcomes()) %>%
  # now we are going to normalise by center and scaling, which is required in
  # lasso regression
  step_normalize(all_numeric(), -all_outcomes())

# prepare the recipe
office_prep <- office_rec %>%
  prep(strings_as_factors = FALSE)
```

Time to build our model: 

```{r}
lasso_spec <- linear_reg(penalty = 0.1, mixture = 1) %>%
  set_engine("glmnet")

# A workflow will help me to put all the pieces together 
wf <- workflow() %>%
  add_recipe(office_rec)

lasso_fit <- wf %>%
  add_model(lasso_spec) %>%
  fit(data = office_train)
  
lasso_fit %>%
  # get the actual results
  pull_workflow_fit() %>%
  tidy() %>% 
  arrange(dev.ratio)
```

#### Tune LASSO parameters

How do we figure out to pick the penalty value. To do that we use resample. To do so, we do a set of resamples:

```{r}
set.seed(1234)
office_boot <- bootstraps(office_train, strata = season)

tune_spec <- linear_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet")

lambda_grid <- grid_regular(penalty(), 
                            levels = 50)

doParallel::registerDoParallel()

set.seed(2020)
lasso_grid <- tune_grid(
  wf %>% add_model(tune_spec), 
  resamples = office_boot, 
  grid = lambda_grid
)
```

```{r}
lasso_grid %>%
  collect_metrics() %>%
  ggplot(aes(penalty, mean, color = .metric)) +
  geom_errorbar(aes(
    ymin = mean - std_err,
    ymax = mean + std_err
  ),
  alpha = 0.5
  ) +
  geom_line(size = 1.5) +
  facet_wrap(~.metric, scales = "free", nrow = 2) +
  scale_x_log10() +
  theme(legend.position = "none")
```

```{r}
lowest_rmse <- lasso_grid %>%
  select_best("rmse")

final_lasso <- finalize_workflow(
  wf %>% add_model(tune_spec),
  lowest_rmse
)

doParallel::stopImplicitCluster()

# let's look at variable importance 
library(vip)

final_lasso %>%
  fit(office_train) %>%
  pull_workflow_fit() %>%
  vi(lambda = lowest_rmse$penalty) %>%
  mutate(
    Importance = abs(Importance),
    Variable = fct_reorder(Variable, Importance)
  ) %>%
  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
  geom_col() +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL)
```

```{r}
last_fit(
  final_lasso,
  office_split
) %>%
  collect_metrics()
```

## Datasaurus data

```{r}
library(tidyverse)
library(datasauRus)

theme_set(theme_light())


datasaurus_dozen %>% 
  ggplot(aes(x, y, color = dataset)) +
  geom_point(alpha = 0.8, show.legend = F) + 
  facet_wrap(~dataset, ncol = 5)
```

```{r}
datasaurus_dozen %>% 
  group_by(dataset) %>% 
  summarise(across(c(x, y), list(mean = mean, sd = sd)), 
            x_y_cor = cor(x, y))
```

You see there are no difference using this descriptive analystics. 
Le'ts look at the count: 

```{r}
datasaurus_dozen %>% 
  count(dataset)
```

Also the same number of observations. 

A good model would be to use a random forest, or any tree model would help us to avoid overfitting. 

## Bluid model

```{r}
library(tidymodels)

dino_folds <- datasaurus_dozen %>%
  # let's see if we can predict the dataset. First transform to factor: 
  mutate(dataset = factor(dataset)) %>% 
  bootstraps()
dino_folds
```

```{r}
rf_spec <- rand_forest(trees = 1000) %>% 
  set_mode("classification") %>% 
  set_engine("ranger")

dino_wf <- workflow() %>% 
  add_model(rf_spec) %>% 
  add_formula(dataset ~ x + y)

doParallel::registerDoParallel()
dino_rs <- fit_resamples(
  dino_wf, 
  resamples = dino_folds, 
  control = control_resamples(save_pred = T)
) 
dino_rs
```

## Evaluate model

```{r}
collect_metrics(dino_rs)
```

```{r}
dino_rs %>% 
  collect_predictions() %>%
  group_by(id) %>% 
  ppv(dataset, .pred_class)
```

```{r}
dino_rs %>% 
  collect_predictions() %>%
  group_by(id) %>%
  roc_curve(dataset, .pred_away:.pred_x_shape) %>% 
  autoplot()
```



```{r}
# look at the confusion matrix
dino_rs %>% 
  collect_predictions() %>% 
  filter(.pred_class != dataset) %>% # filter the correct ones to highlight the wrong classifications
  conf_mat(dataset, .pred_class) %>% 
  autoplot(type = "heatmap")

```

